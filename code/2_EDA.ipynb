{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ethics Disclaimer:** This is the first data science project I've done that deals with peoples opinions and sensitive discussions. You might have noticed during our data cleaning that aside from deleted/removed posts, the top most repeated comment was a copy pasted paragraph denying the Holodomor (the Ukrainian famine that's widely recognized as an act of genocide perpetrated by the USSR). This is probably not the only ugliness that we're going to see.\n",
    "\n",
    "As we explore our data, there's a good chance that we're going to see vile opinions espousing racism and xenophobia, and all sorts of general toxicity, cruelty and ignorance. This is an unfortunate inevitably of processing anonymous political commentary. I'm not a big fan of broadcasting this kind of content, but it's part of the data were dealing with, and understanding the distribution of it between subreddits might become part of our underlying classification metric.\n",
    "\n",
    "## General Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "canada_df=pd.read_csv('..\\data\\canada_subreddit_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>score</th>\n",
       "      <th>body</th>\n",
       "      <th>body_processed</th>\n",
       "      <th>subreddit_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>onguardforthee</td>\n",
       "      <td>nalydpsycho</td>\n",
       "      <td>1600907182</td>\n",
       "      <td>1</td>\n",
       "      <td>I understand what you are saying, what I don't...</td>\n",
       "      <td>I understand what you are saying, what I don't...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>onguardforthee</td>\n",
       "      <td>dexx4d</td>\n",
       "      <td>1600907548</td>\n",
       "      <td>1</td>\n",
       "      <td>Huh, didn't know the owner was like that.\\n\\nS...</td>\n",
       "      <td>Huh, didn't know the owner was like that.\\n\\nS...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>onguardforthee</td>\n",
       "      <td>Man_Bear_Beaver</td>\n",
       "      <td>1600907576</td>\n",
       "      <td>1</td>\n",
       "      <td>love them transfer payments though</td>\n",
       "      <td>love them transfer payments though</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>onguardforthee</td>\n",
       "      <td>whaleoilbeefhookt</td>\n",
       "      <td>1600907722</td>\n",
       "      <td>1</td>\n",
       "      <td>it takes two to tango. tab p in slot v stuff.</td>\n",
       "      <td>it takes two to tango. tab p in slot v stuff.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>onguardforthee</td>\n",
       "      <td>NotInsane_Yet</td>\n",
       "      <td>1600907888</td>\n",
       "      <td>1</td>\n",
       "      <td>It's enough to finance them for a few years.  ...</td>\n",
       "      <td>It's enough to finance them for a few years.  ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20097</th>\n",
       "      <td>canada</td>\n",
       "      <td>toastee</td>\n",
       "      <td>1538658871</td>\n",
       "      <td>184</td>\n",
       "      <td>Can confirm, saw this exact issue on a Yukon r...</td>\n",
       "      <td>Can confirm, saw this exact issue on a Yukon r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20098</th>\n",
       "      <td>canada</td>\n",
       "      <td>FireballSambucca</td>\n",
       "      <td>1538654720</td>\n",
       "      <td>183</td>\n",
       "      <td>She is in the news quite a bit....https://www....</td>\n",
       "      <td>She is in the news quite a bit....https://www....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20099</th>\n",
       "      <td>canada</td>\n",
       "      <td>unseencs</td>\n",
       "      <td>1538666172</td>\n",
       "      <td>180</td>\n",
       "      <td>He was employed?  This story keeps getting str...</td>\n",
       "      <td>He was employed?  This story keeps getting str...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20100</th>\n",
       "      <td>canada</td>\n",
       "      <td>mikailus</td>\n",
       "      <td>1538882519</td>\n",
       "      <td>180</td>\n",
       "      <td>As a pro-choice and pro-free speech guy, great...</td>\n",
       "      <td>As a pro-choice and pro-free speech guy, great...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20101</th>\n",
       "      <td>canada</td>\n",
       "      <td>Im_A_Cringy_Bastard</td>\n",
       "      <td>1539170636</td>\n",
       "      <td>178</td>\n",
       "      <td>To travel to Syria, take up arms against the p...</td>\n",
       "      <td>To travel to Syria, take up arms against the p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20102 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            subreddit               author  created_utc  score  \\\n",
       "0      onguardforthee          nalydpsycho   1600907182      1   \n",
       "1      onguardforthee               dexx4d   1600907548      1   \n",
       "2      onguardforthee      Man_Bear_Beaver   1600907576      1   \n",
       "3      onguardforthee    whaleoilbeefhookt   1600907722      1   \n",
       "4      onguardforthee        NotInsane_Yet   1600907888      1   \n",
       "...               ...                  ...          ...    ...   \n",
       "20097          canada              toastee   1538658871    184   \n",
       "20098          canada     FireballSambucca   1538654720    183   \n",
       "20099          canada             unseencs   1538666172    180   \n",
       "20100          canada             mikailus   1538882519    180   \n",
       "20101          canada  Im_A_Cringy_Bastard   1539170636    178   \n",
       "\n",
       "                                                    body  \\\n",
       "0      I understand what you are saying, what I don't...   \n",
       "1      Huh, didn't know the owner was like that.\\n\\nS...   \n",
       "2                     love them transfer payments though   \n",
       "3          it takes two to tango. tab p in slot v stuff.   \n",
       "4      It's enough to finance them for a few years.  ...   \n",
       "...                                                  ...   \n",
       "20097  Can confirm, saw this exact issue on a Yukon r...   \n",
       "20098  She is in the news quite a bit....https://www....   \n",
       "20099  He was employed?  This story keeps getting str...   \n",
       "20100  As a pro-choice and pro-free speech guy, great...   \n",
       "20101  To travel to Syria, take up arms against the p...   \n",
       "\n",
       "                                          body_processed  subreddit_bin  \n",
       "0      I understand what you are saying, what I don't...              1  \n",
       "1      Huh, didn't know the owner was like that.\\n\\nS...              1  \n",
       "2                     love them transfer payments though              1  \n",
       "3          it takes two to tango. tab p in slot v stuff.              1  \n",
       "4      It's enough to finance them for a few years.  ...              1  \n",
       "...                                                  ...            ...  \n",
       "20097  Can confirm, saw this exact issue on a Yukon r...              0  \n",
       "20098  She is in the news quite a bit....https://www....              0  \n",
       "20099  He was employed?  This story keeps getting str...              0  \n",
       "20100  As a pro-choice and pro-free speech guy, great...              0  \n",
       "20101  To travel to Syria, take up arms against the p...              0  \n",
       "\n",
       "[20102 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "canada_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20102, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "canada_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "onguardforthee    0.50388\n",
       "canada            0.49612\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "canada_df['subreddit'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count              20102\n",
       "unique             10135\n",
       "top       Caucasian_Fury\n",
       "freq                 120\n",
       "Name: author, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "canada_df['author'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9834237789837197"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "canada_df['author'].value_counts().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a dataset of 20,120 comments, with an almost 50/50 split between r/Canada and r/OnGuardForThee sources. We have 10136 distinct comment authors. The most prolific authors in the dataset wrote 120 comments, but the mean author only wrote 2 comments.\n",
    "\n",
    "Aside from text content, I want to know if there's a significant difference in average comment word count per subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "canada_df['word_count']=canada_df['body'].str.count(\"\\w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subreddit\n",
       "canada            42.804773\n",
       "onguardforthee    44.853490\n",
       "Name: word_count, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "canada_df.groupby('subreddit').mean()['word_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/r/OnGuardForThee comments tend to be slightly longer, but by a pretty negligible amount (2 words). The average comment contains 44 words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a quick and dirty analysis of the most commmon words from each subreddit using sklearn tools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_canada=canada_df.loc[canada_df['subreddit']=='canada', 'body']\n",
    "comments_ogft=canada_df.loc[canada_df['subreddit']=='onguardforthee', 'body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_canada = CountVectorizer(stop_words='english', max_features=5000)\n",
    "cv_ogft = CountVectorizer(stop_words='english', max_features=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf_canada=cv_canada.fit_transform(comments_canada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf_ogft=cv_ogft.fit_transform(comments_ogft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf_canada_df=pd.DataFrame(wf_canada.toarray(), columns=cv_canada.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>...</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youre</th>\n",
       "      <th>youth</th>\n",
       "      <th>youtu</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yup</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zero</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9968</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9969</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9970</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9971</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9972</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9973 rows × 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      000  01  02  03  04  05  06  07  08  09  ...  york  young  youre  youth  \\\n",
       "0       0   0   0   0   0   0   0   0   0   0  ...     0      0      0      0   \n",
       "1       0   0   0   0   0   0   0   0   0   0  ...     0      0      0      0   \n",
       "2       0   0   0   0   0   0   0   0   0   0  ...     0      0      0      0   \n",
       "3       0   0   0   0   0   0   0   0   0   0  ...     0      0      0      0   \n",
       "4       0   0   0   0   0   0   0   0   0   0  ...     0      0      0      0   \n",
       "...   ...  ..  ..  ..  ..  ..  ..  ..  ..  ..  ...   ...    ...    ...    ...   \n",
       "9968    0   0   0   0   0   0   0   0   0   0  ...     0      0      0      0   \n",
       "9969    0   0   0   0   0   0   0   0   0   0  ...     0      0      0      0   \n",
       "9970    0   0   0   0   0   0   0   0   0   0  ...     0      0      0      0   \n",
       "9971    0   0   0   0   0   0   0   0   0   0  ...     0      0      0      0   \n",
       "9972    0   0   0   0   0   0   0   0   0   0  ...     0      0      0      0   \n",
       "\n",
       "      youtu  youtube  yup  zealand  zero  zone  \n",
       "0         0        0    0        0     0     0  \n",
       "1         0        0    0        0     0     0  \n",
       "2         0        0    0        0     0     0  \n",
       "3         0        0    0        0     0     0  \n",
       "4         0        0    0        0     0     0  \n",
       "...     ...      ...  ...      ...   ...   ...  \n",
       "9968      0        0    0        0     0     0  \n",
       "9969      0        0    0        0     0     0  \n",
       "9970      0        0    0        0     0     0  \n",
       "9971      0        0    0        0     0     0  \n",
       "9972      0        0    0        0     0     0  \n",
       "\n",
       "[9973 rows x 5000 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wf_canada_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf_ogft_df=pd.DataFrame(wf_ogft.toarray(), columns=cv_ogft.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>...</th>\n",
       "      <th>younger</th>\n",
       "      <th>youre</th>\n",
       "      <th>youth</th>\n",
       "      <th>youtu</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yup</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zero</th>\n",
       "      <th>zone</th>\n",
       "      <th>édition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10124</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10125</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10126</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10127</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10128</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10129 rows × 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       000  01  02  03  04  05  06  07  08  09  ...  younger  youre  youth  \\\n",
       "0        0   0   0   0   0   0   0   0   0   0  ...        0      0      0   \n",
       "1        0   0   0   0   0   0   0   0   0   0  ...        0      0      0   \n",
       "2        0   0   0   0   0   0   0   0   0   0  ...        0      0      0   \n",
       "3        0   0   0   0   0   0   0   0   0   0  ...        0      0      0   \n",
       "4        0   0   0   0   0   0   0   0   0   0  ...        0      0      0   \n",
       "...    ...  ..  ..  ..  ..  ..  ..  ..  ..  ..  ...      ...    ...    ...   \n",
       "10124    0   0   0   0   0   0   0   0   1   0  ...        0      0      0   \n",
       "10125    0   0   0   0   0   0   0   0   0   0  ...        0      0      0   \n",
       "10126    0   0   0   0   0   0   0   0   0   0  ...        0      0      0   \n",
       "10127    0   0   0   0   0   0   0   0   0   0  ...        0      0      0   \n",
       "10128    0   0   0   0   0   0   0   0   0   0  ...        0      0      0   \n",
       "\n",
       "       youtu  youtube  yup  zealand  zero  zone  édition  \n",
       "0          0        0    0        0     0     0        0  \n",
       "1          0        0    0        0     0     0        0  \n",
       "2          0        0    0        0     0     0        0  \n",
       "3          0        0    0        0     0     0        0  \n",
       "4          0        0    0        0     0     0        0  \n",
       "...      ...      ...  ...      ...   ...   ...      ...  \n",
       "10124      0        1    0        0     0     0        0  \n",
       "10125      0        0    0        0     0     0        0  \n",
       "10126      0        0    0        0     0     0        0  \n",
       "10127      0        0    0        0     0     0        0  \n",
       "10128      1        1    0        0     0     0        0  \n",
       "\n",
       "[10129 rows x 5000 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wf_ogft_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf_canada=wf_canada_df.sum()\n",
    "wf_ogft=wf_ogft_df.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at the top 50 words in each subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "people        1963\n",
       "just          1540\n",
       "like          1459\n",
       "don           1359\n",
       "gt            1288\n",
       "canada        1272\n",
       "think          776\n",
       "time           760\n",
       "government     730\n",
       "know           644\n",
       "going          623\n",
       "good           599\n",
       "right          596\n",
       "need           595\n",
       "https          589\n",
       "make           580\n",
       "want           579\n",
       "canadian       556\n",
       "really         539\n",
       "work           538\n",
       "way            535\n",
       "money          521\n",
       "years          515\n",
       "said           512\n",
       "doesn          474\n",
       "ve             471\n",
       "did            467\n",
       "say            442\n",
       "actually       403\n",
       "country        398\n",
       "year           395\n",
       "trudeau        387\n",
       "isn            367\n",
       "thing          361\n",
       "china          359\n",
       "www            358\n",
       "sure           353\n",
       "new            353\n",
       "better         350\n",
       "ll             349\n",
       "pay            345\n",
       "does           344\n",
       "lot            341\n",
       "didn           340\n",
       "canadians      338\n",
       "point          332\n",
       "news           330\n",
       "things         317\n",
       "day            310\n",
       "doing          307\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wf_canada.sort_values(ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "people           2452\n",
       "just             1823\n",
       "like             1727\n",
       "don              1393\n",
       "gt               1358\n",
       "canada           1225\n",
       "right             971\n",
       "think             934\n",
       "https             746\n",
       "time              713\n",
       "know              697\n",
       "going             658\n",
       "make              656\n",
       "want              645\n",
       "good              632\n",
       "government        627\n",
       "really            604\n",
       "way               602\n",
       "need              557\n",
       "ve                516\n",
       "party             501\n",
       "canadian          498\n",
       "say               494\n",
       "conservative      488\n",
       "work              478\n",
       "years             478\n",
       "doesn             462\n",
       "actually          460\n",
       "thing             456\n",
       "isn               448\n",
       "com               446\n",
       "money             434\n",
       "things            432\n",
       "conservatives     430\n",
       "did               429\n",
       "www               428\n",
       "said              428\n",
       "vote              420\n",
       "shit              412\n",
       "lot               403\n",
       "trudeau           402\n",
       "better            398\n",
       "country           386\n",
       "news              378\n",
       "does              367\n",
       "ll                364\n",
       "white             356\n",
       "point             354\n",
       "new               347\n",
       "sure              342\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wf_ogft.sort_values(ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the top 50 tokens in each subreddit tells a few things, both about content and about what our future processing should look like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Refinement\n",
    "\n",
    "- Contractions should be addressed, either by regex or stemming.\n",
    "- The top 5 words in each subreddit are the same: \"people, just, like, don, canada\". Since we're looking to distinguish between the subreddits, I think I might add these to our stopwords so we can have the model focus on more uncommon words.\n",
    "- From the appearance of \"www\", \"com\", and \"http\", we can see that URL's are being tokenized in weird ways. I think we should probably just remove URLs. in processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion differences\n",
    "\n",
    "- /r/OnGuardForThee uses the word \"right\" more than r/Canada. It also has \"conservative\" and \"conservatives\" crack the top 50, while they don't in r/Canada. This might reflect a higher incidence of discussion of right-wing ideology.\n",
    "- /r/Canada uses the word \"government\" more. This may reflect more policy based discussion.\n",
    "- \"pay\" appears in r/Canada's top 50 words, but not in /r/OnGuardForThee. This might correspond to discussions of political budgets.\n",
    "- \"white\" appears in r/OnGuardForThee's top 50 words, but not in r/Canada. This likely reflects a higher incidence of identity politics discussions.\n",
    "- Obscenities crack the top 50 in r/OnGuardForThee, but not r/Canada. This could be an example of differences in discussion tone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a dataframe of some particularly loaded keywords that might come up in discussions of Canadian issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cost          209\n",
       "having        206\n",
       "best          205\n",
       "life          205\n",
       "trying        205\n",
       "business      200\n",
       "means         195\n",
       "aren          195\n",
       "away          194\n",
       "yeah          193\n",
       "guy           192\n",
       "free          192\n",
       "10            192\n",
       "feel          190\n",
       "companies     190\n",
       "ford          190\n",
       "live          189\n",
       "different     189\n",
       "health        189\n",
       "legal         187\n",
       "buy           185\n",
       "political     185\n",
       "working       184\n",
       "liberals      182\n",
       "market        181\n",
       "vote          181\n",
       "man           181\n",
       "hard          179\n",
       "lol           177\n",
       "understand    176\n",
       "support       176\n",
       "company       176\n",
       "needs         175\n",
       "gun           174\n",
       "case          173\n",
       "place         173\n",
       "read          172\n",
       "little        172\n",
       "quebec        171\n",
       "economy       171\n",
       "house         171\n",
       "literally     170\n",
       "believe       170\n",
       "help          169\n",
       "media         167\n",
       "agree         166\n",
       "makes         166\n",
       "able          166\n",
       "says          166\n",
       "start         166\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wf_canada.sort_values(ascending=False).iloc[100:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_politics = ['white', 'black', 'indian', 'native', 'asian', 'chinese',\n",
    "                    'sex', 'race', 'gender', 'racism', 'social', 'justice', 'bias',\n",
    "                    'discrimination', 'men', 'women', 'gay', 'lgbt', 'trans', 'identity']\n",
    "government = ['liberal', 'conservative', 'ndp', 'bloc', 'socialist', 'communist', 'trudeau', 'singh', 'scheer', 'senate',\n",
    "             'scandal', 'parliament', 'mp', 'treaty', 'budget', 'spend', 'election', 'tax', 'party',\n",
    "             'ford', 'vote']\n",
    "\n",
    "issues=['immigration', 'abortion', 'drugs', 'housing', 'covid', 'cerb', 'china', 'trump', 'oil',\n",
    "       'climate', 'warming', 'pipeline', 'gun', 'energy', 'taxes', 'trade', 'police', 'crime', 'military',\n",
    "       'money', 'public', 'world', 'job', 'jobs', 'cost', 'business', 'free', 'health', 'legal', 'working',\n",
    "       'economy', 'market', 'company', 'corporation']\n",
    "\n",
    "geography = ['bc', 'alberta', 'saskatchewan', 'manitoba', 'ontario', 'quebec',\n",
    "            'newfoundland', 'brunswick', 'pei', 'scotia', 'vancouver', 'calgary',\n",
    "            'edmonton', 'winnipeg', 'toronto', 'ottawa', 'province', 'city',\n",
    "            'montreal', 'halifax', 'john', 'east', 'west', 'north']\n",
    "\n",
    "obscene=['shit', 'fuck', 'fucking', 'damn', 'asshole', 'bitch']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords=[]\n",
    "keywords.extend(identity_politics)\n",
    "keywords.extend(government)\n",
    "keywords.extend(issues)\n",
    "keywords.extend(geography)\n",
    "keywords.extend(obscene)\n",
    "\n",
    "categories = {'identity': identity_politics,\n",
    "             'government': government,\n",
    "             'issue': issues,\n",
    "             'geography': geography,\n",
    "             'obscene': obscene}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_map(word):\n",
    "    for key, wordlist in categories.items():\n",
    "        if word in wordlist:\n",
    "            return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>canada_count</th>\n",
       "      <th>ogft_count</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>words</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>white</th>\n",
       "      <td>101</td>\n",
       "      <td>356</td>\n",
       "      <td>identity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>black</th>\n",
       "      <td>83</td>\n",
       "      <td>85</td>\n",
       "      <td>identity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indian</th>\n",
       "      <td>16</td>\n",
       "      <td>22</td>\n",
       "      <td>identity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>native</th>\n",
       "      <td>35</td>\n",
       "      <td>38</td>\n",
       "      <td>identity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asian</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>identity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fuck</th>\n",
       "      <td>295</td>\n",
       "      <td>331</td>\n",
       "      <td>obscene</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fucking</th>\n",
       "      <td>219</td>\n",
       "      <td>315</td>\n",
       "      <td>obscene</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>damn</th>\n",
       "      <td>75</td>\n",
       "      <td>60</td>\n",
       "      <td>obscene</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asshole</th>\n",
       "      <td>19</td>\n",
       "      <td>39</td>\n",
       "      <td>obscene</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bitch</th>\n",
       "      <td>11</td>\n",
       "      <td>26</td>\n",
       "      <td>obscene</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         canada_count  ogft_count  category\n",
       "words                                      \n",
       "white             101         356  identity\n",
       "black              83          85  identity\n",
       "indian             16          22  identity\n",
       "native             35          38  identity\n",
       "asian              11          11  identity\n",
       "...               ...         ...       ...\n",
       "fuck              295         331   obscene\n",
       "fucking           219         315   obscene\n",
       "damn               75          60   obscene\n",
       "asshole            19          39   obscene\n",
       "bitch              11          26   obscene\n",
       "\n",
       "[105 rows x 3 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_df=pd.DataFrame()\n",
    "keyword_df['words']=keywords\n",
    "keyword_df['canada_count'] = wf_canada[keywords].values\n",
    "keyword_df['ogft_count'] = wf_ogft[keywords].values\n",
    "keyword_df['category']=keyword_df['words'].map(category_map)\n",
    "keyword_df.set_index('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "canada_count    11817\n",
       "ogft_count      13586\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_df[['canada_count', 'ogft_count']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>canada_count</th>\n",
       "      <th>ogft_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>geography</th>\n",
       "      <td>1970</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>government</th>\n",
       "      <td>2384</td>\n",
       "      <td>3723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>identity</th>\n",
       "      <td>1101</td>\n",
       "      <td>1892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>issue</th>\n",
       "      <td>5441</td>\n",
       "      <td>4788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>obscene</th>\n",
       "      <td>921</td>\n",
       "      <td>1183</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            canada_count  ogft_count\n",
       "category                            \n",
       "geography           1970        2000\n",
       "government          2384        3723\n",
       "identity            1101        1892\n",
       "issue               5441        4788\n",
       "obscene              921        1183"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_df.groupby('category').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, my keywords hit more occurrences in r/OnGuardForThee than r/Canada. I tried to select keywords that would show up around equally in both, but wasn't able to find a set that gave me an exact balance and still held semantic meaning. This could indicate two things\n",
    "- I just kinda made up the keywords myself by what I thought would be relevent while cross referencing top words in both subreddits. The bias towards OGFT in word occurance might just reflect my own political/discussion leanings.\n",
    "- OGFT might engage in more specific, technical discussion than r/Canada about policy and social issues. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "r/OnGuardForThee dominates wordcounts that relate to government issues, and identity politics issues. It has a slight edge in obscenity.\n",
    "\n",
    "r/Canada on the other hand has a slight edge in the \"issue\" category that encompasses a wide range of general issue keywords that include business concerns and more general social concerns."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

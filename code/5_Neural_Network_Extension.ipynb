{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "snow=SnowballStemmer(\"english\")\n",
    "def snowball_tokens2(text):\n",
    "    text_processed = re.sub(r'[^A-Za-z]', ' ', text).split()\n",
    "    tokens = [snow.stem(word) for word in text_processed]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "canada_df = pd.read_csv('..\\data\\canada_subreddit_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stopwords= stopwords.words('english')\n",
    "custom_stopwords.extend(['people', 'like', 'canada'])\n",
    "custom_stopwords = [snow.stem(word) for word in custom_stopwords]\n",
    "#I'm cheating and adding a few more stopwords here that I identfied as highly shared between both subreddits\n",
    "#that I didn't identify until after doing some more analysis on top tokens\n",
    "\n",
    "extra_stopwords=['get', 'would', 'gt', 'one', 'go', 'make', \n",
    "                 'actual', 'also', 'back', 'us', 'use', 'could', 'say', 'said', 'see', 'back', 'come',\n",
    "                'canadian', 'look']\n",
    "\n",
    "custom_stopwords.extend(extra_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Embedding, Conv1D\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=canada_df['body_processed']\n",
    "y=canada_df['subreddit_bin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comments=[]\n",
    "\n",
    "for comment in X:\n",
    "    token_list=[x for x in snowball_tokens2(comment) if x not in custom_stopwords]\n",
    "    comments.append(\" \".join(token_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(comments, y, random_state=42, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16081 4021\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train), len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=4000 #max number of words\n",
    "embedding_dim=32 #word vector dimension\n",
    "max_length= 50 # max length of sequence (sentence)\n",
    "trunc_type = 'post' #where to truncate if over max length (this cuts all vals after 200)\n",
    "padding_type = \"post\" # where to add padding -- this adds padding to end\n",
    "oov_tok='<00V>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequence=tokenizer.texts_to_sequences(X_train)\n",
    "train_padded=pad_sequences(train_sequence, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequence= tokenizer.texts_to_sequences(X_test)\n",
    "test_padded=pad_sequences(test_sequence, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_48\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_48 (Embedding)     (None, 50, 32)            128000    \n",
      "_________________________________________________________________\n",
      "lstm_51 (LSTM)               (None, 4)                 592       \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 128,597\n",
      "Trainable params: 128,597\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model= Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=max_length))\n",
    "model.add(LSTM(4, activity_regularizer=regularizers.l2(5)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(loss = 'binary_crossentropy', optimizer=opt, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop=EarlyStopping(monitor='val_acc', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_acc', patience=5, factor=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 0.6962 - acc: 0.5049 - val_loss: 0.6934 - val_acc: 0.5068\n",
      "Epoch 2/1000\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.6927 - acc: 0.5285 - val_loss: 0.6933 - val_acc: 0.5068\n",
      "Epoch 3/1000\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 0.6923 - acc: 0.5395 - val_loss: 0.6934 - val_acc: 0.5058\n",
      "Epoch 4/1000\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.6920 - acc: 0.5442 - val_loss: 0.6935 - val_acc: 0.5061\n",
      "Epoch 5/1000\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 0.6915 - acc: 0.5472 - val_loss: 0.6938 - val_acc: 0.5081\n",
      "Epoch 6/1000\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 0.6911 - acc: 0.5531 - val_loss: 0.6941 - val_acc: 0.5091\n",
      "Epoch 7/1000\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 0.6905 - acc: 0.5624 - val_loss: 0.6949 - val_acc: 0.5210\n",
      "Epoch 8/1000\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 0.6898 - acc: 0.5837 - val_loss: 0.6957 - val_acc: 0.5190\n",
      "Epoch 9/1000\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 0.6883 - acc: 0.6120 - val_loss: 0.6959 - val_acc: 0.5476\n",
      "Epoch 10/1000\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 0.6870 - acc: 0.6360 - val_loss: 0.6962 - val_acc: 0.5732\n",
      "Epoch 11/1000\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.6843 - acc: 0.6671 - val_loss: 0.6970 - val_acc: 0.5730\n",
      "Epoch 12/1000\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 0.6821 - acc: 0.6851 - val_loss: 0.6979 - val_acc: 0.5889\n",
      "Epoch 13/1000\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 0.6809 - acc: 0.6874 - val_loss: 0.7000 - val_acc: 0.5583\n",
      "Epoch 14/1000\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.6799 - acc: 0.6835 - val_loss: 0.6997 - val_acc: 0.5650\n",
      "Epoch 15/1000\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.6760 - acc: 0.7153 - val_loss: 0.6993 - val_acc: 0.5785\n",
      "Epoch 16/1000\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.6739 - acc: 0.7212 - val_loss: 0.7011 - val_acc: 0.5807\n",
      "Epoch 17/1000\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.6729 - acc: 0.7139 - val_loss: 0.7005 - val_acc: 0.5772\n",
      "Epoch 18/1000\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.6688 - acc: 0.7374 - val_loss: 0.7044 - val_acc: 0.5812\n",
      "Epoch 19/1000\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.6673 - acc: 0.7460 - val_loss: 0.7054 - val_acc: 0.5879\n",
      "Epoch 20/1000\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.6667 - acc: 0.7493 - val_loss: 0.7050 - val_acc: 0.5867\n",
      "Epoch 21/1000\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.6661 - acc: 0.7485 - val_loss: 0.7064 - val_acc: 0.5877\n",
      "Epoch 22/1000\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.6654 - acc: 0.7506 - val_loss: 0.7067 - val_acc: 0.5800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x212f57845e0>"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.fit(train_padded, y_train, validation_data=(test_padded, y_test), batch_size=512, epochs=1000, callbacks=[early_stop, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
